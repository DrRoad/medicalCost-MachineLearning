summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Class)
mean(pred == test$Class)
library(neuralnet)
nn1 <- neuralnet(Class~., data=train)
library(neuralnet)
nn1 <- neuralnet(Class~V1+V4+V5+V8+V9+V10+V13+V14+V20+V21+V22+V27+V28+Amount., data=train)
library(neuralnet)
nn1 <- neuralnet(Class~V1+V4+V5+V8+V9+V10+V13+V14+V20+V21+V22+V27+V28+Amount, data=train)
results <- compute(nn1, test[1:31])
library(caret)
fit <- knnreg(train[,1:30],train[,31],k=3)
predictions <- predict(fit, test[,1:30])
cor(predictions, test$Class)
lm1 <- lm(Class~., data=train)
dataframe1 <- read.csv(file="creditcard.csv", header=TRUE)
set.seed(2017)
i <- sample(nrow(dataframe1), nrow(dataframe1)*0.75, replace=FALSE)
train <- dataframe1[i,]
test <- dataframe1[-i,]
lm1 <- lm(Class~., data=train)
lm1 <- lm(Class~., data=train)
pred <- predict(lm1, newdata=test)
cor(pred, test$Class)
head(pred)
tail(pred)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
glm1 = glm(Hospitalized~., data=train, family=binomial)
library(tree)
tree.carseats = tree(Hospita~., train) # omit Sales
library(tree)
tree.carseats = tree(Hospitalized~., train) # omit Sales
View(injuredDataset)
View(injuredDataset)
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.body, data=train, family=binomial)
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$default)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 0, 1)
table(pred, test$Hospitalized)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
glm1 <- glm(Hospitalized~Part.of.Body, data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
mean(pred == test$Hospitalized)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
mean(pred == test$Hospitalized)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
library(tree)
High = ifelse(Hospitalized<=0.5, 0,1)
library(tree)
High = ifelse(injuredDataset$Hospitalized<=0.5, 0,1)
Carseats = data.frame(injuredDataset, High)
tree.carseats = tree(High~., Carseats)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
mean(pred == test$Hospitalized)
View(injuredDataset)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(df[,-5], df[,5], data=train)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
View(injuredDataset)
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(df[,-5], df[,5], data=train)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(injuredDataset[,-5], df[,5], data=train)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(injuredDataset[,-5], injuredDataset[,5], data=train)
View(injuredDataset)
View(injuredDataset)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
mean(pred == test$Hospitalized)
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(injuredDataset[,-5], injuredDataset[,5], data=train)
nb1
p1 <- predict(nb1, newdata=test[,-5], type="class")
table(p1, test[,5])
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(injuredDataset[,-5], injuredDataset[,5], data=train)
nb1
p1 <- predict(nb1, newdata=test[,-5], type="class")
table(p1, test[,5])
mean(p2 == test[,5])
injuredDataset$Hospitalized = as.factor(injuredDataset$Hospitalized)
library(e1071)
nb1 <- naiveBayes(injuredDataset[,-5], injuredDataset[,5], data=train)
nb1
p1 <- predict(nb1, newdata=test[,-5], type="class")
table(p1, test[,5])
mean(p1 == test[,5])
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
View(train)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
creditnet <- neuralnet(Hospitalized ~ Zip+Latitude+Longitude+Ampulation+Part.of.Body, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip+Latitude+Longitude+Ampulation+Part.of.Body, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip+Latitude+Longitude+Amputation+Part.of.Body, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip+Latitude+Longitude+Amputation+Part.of.Body, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip+Latitude+Longitude, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
train$Zip <- as.factor(train$Zip)
train$Hospitalized <- as.factor(train$Hospitalized)
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
#load
injuredDataset <- read.csv(file = "severeinjury.csv", header = TRUE)
injuredDataset <- na.omit(injuredDataset)
#explore
str(injuredDataset)
head(injuredDataset)
tail(injuredDataset)
#filtering th data
injuredDataset$Hospitalized = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
#dropping the columns
injuredDataset <- injuredDataset[,-c(1:8, 15, 16, 18, 20:26)]
#Train and test set
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
glm1 <- glm(Hospitalized~Zip+Latitude+Longitude+Part.of.Body, data=train, family=binomial)
summary(glm1)
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
table(pred, test$Hospitalized)
mean(pred == test$Hospitalized)
i <- sample(nrow(injuredDataset), nrow(injuredDataset)*0.75, replace=FALSE)
train <- injuredDataset[i,]
test <- injuredDataset[-i,]
train$Zip <- as.factor(train$Zip)
train$Hospitalized <- as.factor(train$Hospitalized)
library(neuralnet)
creditnet <- neuralnet(Hospitalized ~ Zip, train,
hidden=4, lifesign="minimal",
linear.output=FALSE, threshold=0.1)
High = ifelse(Sales<=0.5,0,1)
High = ifelse(Hospitalized<=0.5,0,1)
tree.carseats = tree(Hospitalized~., injuredDataset) # omit Sales
train1 <- injuredDataset
train$Zip <- NULL
model<-tree(Hospitalized~.,train)
injuredDataset$HospOrNot = ifelse(injuredDataset$Hospitalized == 0, 0, 1)
View(injuredDataset)
injuredDataset <- injuredDataset[,-c(4)]
set.seed(1958)  # setting a seed gets the same results every time
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris.train <- iris[ind==1, 1:7]
set.seed(1958)  # setting a seed gets the same results every time
ind <- sample(2, nrow(injuredDataset), replace=TRUE, prob=c(0.67, 0.33))
injured.train <- injuredDataset[ind==1, 1:7]
injured.test <- injuredDataset[ind==2, 1:7]
injured.trainLabels <- injuredDataset[ind==1, 8]
injured.testLabels <- injuredDataset[ind==2, 8]
library(class)
injuredpred <- knn(train = injured.train, test=injured.test, cl = injured.trainLabels, k=3)
results <- injured_pred == injured.testLabels
results <- injuredpred == injured.testLabels
acc <- length(which(results==TRUE)) / length(results)
acc
toxic=read.csv("train.csv",header=TRUE, sep=",", quote='\"\"', stringsAsFactors=FALSE)
toxic <- toxic[-c(1)]
toxic <- toxic[-c(3:7)]
toxic$comment = toxic$comment_text
toxic <- toxic[-c(1)]
library(tm)
names(toxic)<-c("type","message")
msgCorpos = Corpus(VectorSource(toxic$message))
inspect(msgCorpos[1])
toxicClean <- tm_map(msgCorpos, content_transformer(tolower))
toxicClean <- tm_map(toxicClean, removeNumbers)
toxicClean <- tm_map(toxicClean, removePunctuation)
toxicClean <- tm_map(toxicClean, stripWhitespace)
toxic_dtm <- DocumentTermMatrix(toxicClean)
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
toxic_test <- DocumentTermMatrix(toxic_test,
control=list(dictionary=freq_words))
inspect(toxic_train[50:55,200:208])
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
library(e1071)
sms_classifier <- naiveBayes(toxic_train, factor(toxic_label))
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
freq_words <- findFreqTerms(toxic_dtm, 5)
#freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
#freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
freq_words <- findFreqTerms(toxic_dtm, 5)
library(tm)
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
toxic_test <- DocumentTermMatrix(toxic_test,
control=list(dictionary=freq_words))
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic=read.csv("train.csv",header=TRUE, sep=",", quote='\"\"', stringsAsFactors=FALSE)
toxic <- toxic[-c(1)]
toxic <- toxic[-c(3:7)]
toxic <- toxic[-c(1)]
toxic$comment = toxic$comment_text
toxic=read.csv("train.csv",header=TRUE, sep=",", quote='\"\"', stringsAsFactors=FALSE)
toxic <- toxic[-c(1)]
toxic <- toxic[-c(3:7)]
toxic$comment = toxic$comment_text
toxic <- toxic[-c(1)]
library(tm)
names(toxic)<-c("type","message")
msgCorpos = Corpus(VectorSource(toxic$message))
toxic_dtm <- DocumentTermMatrix(toxicClean)
inspect(msgCorpos[1])
toxicClean <- tm_map(msgCorpos, content_transformer(tolower))
toxicClean <- tm_map(toxicClean, removeNumbers)
toxicClean <- tm_map(toxicClean, removePunctuation)
toxicClean <- tm_map(toxicClean, stripWhitespace)
toxic_dtm <- DocumentTermMatrix(toxicClean)
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
toxic_test <- DocumentTermMatrix(toxic_test,
control=list(dictionary=freq_words))
inspect(toxic_train[50:55,200:208])
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
toxic=read.csv("train.csv",header=TRUE, sep=",", quote='\"\"', stringsAsFactors=FALSE)
toxic <- toxic[-c(1)]
toxic <- toxic[-c(3:7)]
toxic$comment = toxic$comment_text
toxic <- toxic[-c(1)]
library(tm)
names(toxic)<-c("type","message")
msgCorpos = Corpus(VectorSource(toxic$message))
inspect(msgCorpos[1])
toxicClean <- tm_map(msgCorpos, content_transformer(tolower))
toxicClean <- tm_map(toxicClean, removeNumbers)
toxicClean <- tm_map(toxicClean, removePunctuation)
toxicClean <- tm_map(toxicClean, stripWhitespace)
toxic_dtm <- DocumentTermMatrix(toxicClean)
set.seed(1958)
len_df = nrow(toxic)
train_ind <- sample(seq_len(len_df), size=floor(0.75*len_df))
train_label <- toxic[train_ind,1]    # column 1 is ham or spam
test_label <- toxic[-train_ind,1]
toxic_train <- toxicClean[train_ind]
toxic_test <- toxicClean[-train_ind]
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y)
y
}
toxic_train <- apply(toxic_train, 2, convert_count)
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
toxic_test <- DocumentTermMatrix(toxic_test,
control=list(dictionary=freq_words))
inspect(toxic_train[50:55,200:208])
View(toxic_train)
View(toxic_test)
freq_words <- findFreqTerms(toxic_dtm, 5)
toxic_train <- DocumentTermMatrix(toxic_train,
control=list(dictionary=freq_words))
library(e1071)
sms_classifier <- naiveBayes(toxic_train, factor(toxic_label))
toxic_train <- as.data.frame(as.matrix(toxic_train))
library(RTextTools)
#data(USCongress)
toxic = read.csv(file="train.csv", header = TRUE)
toxic <- toxic[1:10000,]
toxic <- toxic[-c(1)]
toxic <- toxic[-c(3:7)]
toxic$comment = toxic$comment_text
toxic <- toxic[-c(1)]
# CREATE THE DOCUMENT-TERM MATRIX
doc_matrix <- create_matrix(toxic$comment, language="english", removeNumbers=TRUE,
stemWords=TRUE, removeSparseTerms=.998)
container <- create_container(doc_matrix, toxic$toxic, trainSize=1:7500,
testSize=7501:10000, virgin=FALSE)
SVM <- train_model(container,"SVM")
#RF <- train_model(container,"RF")
SVM_CLASSIFY <- classify_model(container, SVM)
#RF_CLASSIFY <- classify_model(container, RF)
analytics <- create_analytics(container,
cbind(SVM_CLASSIFY))
summary(analytics)
topic_summary <- analytics@label_summary
alg_summary <- analytics@algorithm_summary
ens_summary <-analytics@ensemble_summary
doc_summary <- analytics@document_summary
SVM <- cross_validate(container, 4, "SVM")
analytics@document_summary
#write.csv(analytics@document_summary, file = "DocumentSummary.csv")
toxic = read.csv(file="train.csv", header = TRUE)
View(toxic)
View(toxic)
insuranceDataset = read.csv("insurance.csv", header = TRUE)
setwd("C:/Users/Faraz Khalid/Desktop/Faraz Khalid/GithubFiles/medicalCost-MachineLearning")
setwd("C:/Users/Faraz Khalid/Desktop/Faraz Khalid/GithubFiles")
insuranceDataset = read.csv("insurance.csv", header = TRUE)
setwd("C:/Users/Faraz Khalid/Desktop/Faraz Khalid/GithubFiles/medicalCost-MachineLearning")
insuranceDataset = read.csv("insurance.csv", header = TRUE)
insuranceDataset = read.csv("insurance.csv", header = TRUE)
head(insuranceDataset)
